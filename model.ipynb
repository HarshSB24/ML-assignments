{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "[2025-04-08 20:38:05,483] [ WARNING] module_wrapper.py:149 - From c:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/04/08 20:38:08] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Harsh/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Harsh/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\Harsh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\Harsh/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "\n",
      "=== Prescription Processing System ===\n",
      "1. Process prescription image\n",
      "2. Train model\n",
      "3. Exit\n",
      "\n",
      "=== Model Training ===\n",
      "Error: No image files found in the dataset directory!\n",
      "\n",
      "=== Prescription Processing System ===\n",
      "1. Process prescription image\n",
      "2. Train model\n",
      "3. Exit\n",
      "\n",
      "=== Model Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch (50 images):  22%|██▏       | 11/50 [00:09<00:30,  1.28it/s][2025-04-08 20:39:34,284] [ WARNING] 3241253954.py:229 - No OCR results for D:\\Coding\\Machine_Learning\\Projects\\VAMD\\\\dataset\\\\train\\\\resized_images\\1008.png\n",
      "Processing batch (50 images):  60%|██████    | 30/50 [00:25<00:16,  1.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1004\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1004\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 883\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    875\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    876\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m    877\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    878\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m    879\u001b[0m ])\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPrescriptionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mocr_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;66;03m# Check if dataset is empty\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m, in \u001b[0;36mPrescriptionDataset.__init__\u001b[1;34m(self, image_dir, cache_dir, transform, force_reload, batch_process, batch_size, image_size)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reload \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_cache_exists():\n\u001b[0;32m     97\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing images and caching results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Load cached results\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cached_results()\n",
      "Cell \u001b[1;32mIn[1], line 149\u001b[0m, in \u001b[0;36mPrescriptionDataset._preprocess_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Process image\u001b[39;00m\n\u001b[0;32m    148\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, img_file)\n\u001b[1;32m--> 149\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Cache result\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[1], line 219\u001b[0m, in \u001b[0;36mPrescriptionDataset._process_single_image\u001b[1;34m(self, image_path, ocr)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Perform OCR with optimized parameters\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Force garbage collection after OCR to free memory\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\paddleocr.py:766\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[1;34m(self, img, det, rec, cls, bin, inv, alpha_color, slice)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[0;32m    765\u001b[0m     img \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[1;32m--> 766\u001b[0m     dt_boxes, rec_res, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dt_boxes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rec_res:\n\u001b[0;32m    768\u001b[0m         ocr_res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\predict_system.py:129\u001b[0m, in \u001b[0;36mTextSystem.__call__\u001b[1;34m(self, img, cls, slice)\u001b[0m\n\u001b[0;32m    127\u001b[0m tmp_box \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(dt_boxes[bno])\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdet_box_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquad\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 129\u001b[0m     img_crop \u001b[38;5;241m=\u001b[39m \u001b[43mget_rotate_crop_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mori_im\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     img_crop \u001b[38;5;241m=\u001b[39m get_minarea_rect_crop(ori_im, tmp_box)\n",
      "File \u001b[1;32mc:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paddleocr\\tools\\infer\\utility.py:730\u001b[0m, in \u001b[0;36mget_rotate_crop_image\u001b[1;34m(img, points)\u001b[0m\n\u001b[0;32m    721\u001b[0m pts_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(\n\u001b[0;32m    722\u001b[0m     [\n\u001b[0;32m    723\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    727\u001b[0m     ]\n\u001b[0;32m    728\u001b[0m )\n\u001b[0;32m    729\u001b[0m M \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetPerspectiveTransform(points, pts_std)\n\u001b[1;32m--> 730\u001b[0m dst_img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpPerspective\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_crop_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_crop_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mborderMode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBORDER_REPLICATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_CUBIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m dst_img_height, dst_img_width \u001b[38;5;241m=\u001b[39m dst_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dst_img_height \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m dst_img_width \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from functools import lru_cache\n",
    "import uuid\n",
    "import spacy\n",
    "from gtts import gTTS\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "try:\n",
    "    nltk.data.find('chunkers/maxent_ne_chunker')\n",
    "except LookupError:\n",
    "    nltk.download('maxent_ne_chunker')\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "\n",
    "# Load spaCy model for NLP\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize scheduler\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(levelname)8s] %(filename)s:%(lineno)d - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('prescription_process.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize NLP components\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "class PrescriptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for prescription images\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, transform=None, cache_dir=\"ocr_cache\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Set up image and label directories\n",
    "        self.images_dir = os.path.join(image_dir, \"resized_images\")\n",
    "        self.labels_dir = os.path.join(image_dir, \"labels\")\n",
    "        \n",
    "        if not os.path.exists(self.images_dir) or not os.path.exists(self.labels_dir):\n",
    "            raise ValueError(f\"Required directories not found in {image_dir}. Expected 'resized_images' and 'labels' directories.\")\n",
    "        \n",
    "        # Get all image files from the resized_images directory\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Load cached results\n",
    "        self.cached_results = self._load_cached_results()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        self.print_statistics()\n",
    "    \n",
    "    def _check_cache_exists(self):\n",
    "        \"\"\"\n",
    "        Check if cache exists for all images\n",
    "        \"\"\"\n",
    "        for img_file in self.image_files:\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "            if not os.path.exists(cache_path):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _load_cached_results(self):\n",
    "        \"\"\"\n",
    "        Load cached OCR results and labels\n",
    "        \"\"\"\n",
    "        cached_results = {}\n",
    "        for img_file in self.image_files:\n",
    "            # Try to load from cache first\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "            label_path = os.path.join(self.labels_dir, f\"{os.path.splitext(img_file)[0]}.json\")\n",
    "            \n",
    "            try:\n",
    "                if os.path.exists(cache_path):\n",
    "                    with open(cache_path, 'r') as f:\n",
    "                        cached_results[img_file] = json.load(f)\n",
    "                elif os.path.exists(label_path):\n",
    "                    # If no cache exists but label file does, load from label file\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        label_data = json.load(f)\n",
    "                        cached_results[img_file] = {\n",
    "                            'label': label_data.get('medicine_types', [0] * 7),\n",
    "                            'medicines': label_data.get('medicines', [])\n",
    "                        }\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading data for {img_file}: {str(e)}\")\n",
    "                # Set default values if loading fails\n",
    "                cached_results[img_file] = {'label': [0] * 7, 'medicines': []}\n",
    "                continue\n",
    "                \n",
    "        return cached_results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_file)\n",
    "        \n",
    "        # Load image with PIL and handle memory issue\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            # Return a black image in case of error\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Get cached result\n",
    "        result = self.cached_results.get(img_file, {'label': [0] * 7})\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(result['label'], dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"Print dataset statistics with proper error handling\"\"\"\n",
    "        logger.info(\"\\nDataset Statistics:\")\n",
    "        logger.info(f\"Total images: {len(self.image_files)}\")\n",
    "        logger.info(f\"Cached results: {len(self.cached_results)}\")\n",
    "        \n",
    "        # Print sample label distribution\n",
    "        labels = [result['label'] for result in self.cached_results.values()]\n",
    "        if not labels:\n",
    "            logger.warning(\"No labels found in the dataset!\")\n",
    "            return\n",
    "            \n",
    "        labels = np.array(labels)\n",
    "        if labels.size == 0:\n",
    "            logger.warning(\"Empty labels array!\")\n",
    "            return\n",
    "            \n",
    "        if len(labels.shape) < 2:\n",
    "            logger.warning(f\"Invalid labels shape: {labels.shape}\")\n",
    "            return\n",
    "            \n",
    "        logger.info(\"\\nLabel Distribution:\")\n",
    "        medicine_types = [\"Tablets\", \"Capsules\", \"Syrups\", \"Injections\", \"Drops\", \"Creams\", \"Inhalers\"]\n",
    "        for i, med_type in enumerate(medicine_types):\n",
    "            positive_count = np.sum(labels[:, i] == 1)\n",
    "            logger.info(f\"{med_type}: {positive_count} positive samples ({positive_count/len(labels)*100:.2f}%)\")\n",
    "\n",
    "class PrescriptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch model for prescription medicine classification\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(PrescriptionModel, self).__init__()\n",
    "        \n",
    "        # Use a pre-trained ResNet model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Modify the final layer for our classification task\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()  # Use sigmoid for multi-label classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class PrescriptionTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for the prescription model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, device, learning_rate=1e-4):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            images = batch['image'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        f1 = f1_score(all_labels, np.array(all_preds) > 0.5, average='weighted')\n",
    "        \n",
    "        return avg_loss, f1\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                images = batch['image'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Track metrics\n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        f1 = f1_score(all_labels, np.array(all_preds) > 0.5, average='weighted')\n",
    "        \n",
    "        return avg_loss, f1\n",
    "    \n",
    "    def train(self, num_epochs=10, patience=5):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_f1 = self.train_epoch()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_f1 = self.validate()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Log metrics\n",
    "            logger.info(f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            logger.info(f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), 'best_prescription_model.pth')\n",
    "                logger.info(\"Model saved!\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "class NLPTextExtractor:\n",
    "    \"\"\"\n",
    "    Class for extracting medicine information from prescription text using NLP techniques\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.medicine_types = {\n",
    "            'tablet': ['tab', 'tablet', 'pill'],\n",
    "            'capsule': ['cap', 'capsule'],\n",
    "            'syrup': ['syrup', 'suspension', 'syr'],\n",
    "            'injection': ['inj', 'injection'],\n",
    "            'drops': ['drops', 'eye drops'],\n",
    "            'cream': ['cream', 'ointment'],\n",
    "            'inhaler': ['inhaler', 'spray']\n",
    "        }\n",
    "        \n",
    "        # Common dosage units\n",
    "        self.dosage_units = ['mg', 'g', 'ml', 'mcg', 'tablets', 'tablet', 'capsules', 'capsule']\n",
    "        \n",
    "        # Common frequency patterns\n",
    "        self.frequency_patterns = [\n",
    "            'once a day', 'twice a day', 'three times a day', 'four times a day',\n",
    "            '1x', '2x', '3x', '4x', '1 time', '2 times', '3 times', '4 times'\n",
    "        ]\n",
    "        \n",
    "        # Common duration patterns\n",
    "        self.duration_patterns = [\n",
    "            'days', 'weeks', 'months', 'day', 'week', 'month'\n",
    "        ]\n",
    "    \n",
    "    def extract_medicine_data(self, text):\n",
    "        \"\"\"\n",
    "        Extract medicine information from text using spaCy NLP\n",
    "        \"\"\"\n",
    "        medicine_data = []\n",
    "        \n",
    "        # Split text into lines\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Process each line to identify medicine sections\n",
    "        current_medicine = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if this line starts a new medicine section\n",
    "            is_new_medicine = False\n",
    "            for med_types in self.medicine_types.values():\n",
    "                for med_type in med_types:\n",
    "                    if line.lower().startswith(med_type.lower()):\n",
    "                        is_new_medicine = True\n",
    "                        break\n",
    "                if is_new_medicine:\n",
    "                    break\n",
    "            \n",
    "            if is_new_medicine:\n",
    "                # If we have a current medicine, add it to the list\n",
    "                if current_medicine and current_medicine[\"name\"]:\n",
    "                    medicine_data.append(current_medicine)\n",
    "                \n",
    "                # Start a new medicine entry\n",
    "                current_medicine = {\n",
    "                    \"name\": \"\",\n",
    "                    \"dosage\": \"\",\n",
    "                    \"frequency\": \"\",\n",
    "                    \"duration\": \"\"\n",
    "                }\n",
    "                \n",
    "                # Extract medicine name from this line\n",
    "                parts = line.split()\n",
    "                for i, part in enumerate(parts):\n",
    "                    if any(med_type in part.lower() for med_types in self.medicine_types.values() for med_type in med_types):\n",
    "                        if i + 1 < len(parts):\n",
    "                            # Join remaining parts as the medicine name\n",
    "                            name_parts = parts[i+1:]\n",
    "                            current_medicine[\"name\"] = ' '.join(name_parts).replace(' - ', '-').strip()\n",
    "                            break\n",
    "            \n",
    "            # If we have a current medicine, try to extract other information\n",
    "            if current_medicine:\n",
    "                # Check for dosage\n",
    "                if \"tablets\" in line.lower() or \"tablet\" in line.lower() or \"ml\" in line.lower():\n",
    "                    # Extract dosage\n",
    "                    doc = nlp(line)\n",
    "                    for token in doc:\n",
    "                        if token.like_num:\n",
    "                            try:\n",
    "                                quantity = int(token.text)\n",
    "                                # Look for the unit\n",
    "                                if token.i + 1 < len(doc):\n",
    "                                    unit = doc[token.i+1].text.lower()\n",
    "                                    if \"tablet\" in unit or \"ml\" in unit:\n",
    "                                        current_medicine[\"dosage\"] = f\"{quantity} {unit}\"\n",
    "                                        break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                \n",
    "                # Check for frequency\n",
    "                if \"times\" in line.lower() or \"once\" in line.lower() or \"twice\" in line.lower():\n",
    "                    # Extract frequency\n",
    "                    doc = nlp(line)\n",
    "                    for token in doc:\n",
    "                        if token.like_num:\n",
    "                            try:\n",
    "                                times = int(token.text)\n",
    "                                if token.i + 1 < len(doc) and \"times\" in doc[token.i+1].text.lower():\n",
    "                                    current_medicine[\"frequency\"] = f\"{times} times a day\"\n",
    "                                    break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    # Check for text-based frequency\n",
    "                    if not current_medicine[\"frequency\"]:\n",
    "                        if \"once\" in line.lower():\n",
    "                            current_medicine[\"frequency\"] = \"once a day\"\n",
    "                        elif \"twice\" in line.lower():\n",
    "                            current_medicine[\"frequency\"] = \"twice a day\"\n",
    "                        elif \"three times\" in line.lower():\n",
    "                            current_medicine[\"frequency\"] = \"three times a day\"\n",
    "                \n",
    "                # Check for duration\n",
    "                if \"days\" in line.lower() or \"day\" in line.lower():\n",
    "                    # Extract duration\n",
    "                    doc = nlp(line)\n",
    "                    for token in doc:\n",
    "                        if token.like_num:\n",
    "                            try:\n",
    "                                days = int(token.text)\n",
    "                                if token.i + 1 < len(doc) and \"day\" in doc[token.i+1].text.lower():\n",
    "                                    current_medicine[\"duration\"] = f\"{days} days\"\n",
    "                                    break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "        \n",
    "        # Add the last medicine if it exists\n",
    "        if current_medicine and current_medicine[\"name\"]:\n",
    "            medicine_data.append(current_medicine)\n",
    "        \n",
    "        return medicine_data\n",
    "\n",
    "class ReminderGenerator:\n",
    "    \"\"\"\n",
    "    Class for generating and scheduling reminders based on extracted medicine data\n",
    "    \"\"\"\n",
    "    def __init__(self, user_id, prescription_id=None, config=None):\n",
    "        self.user_id = user_id\n",
    "        self.prescription_id = prescription_id\n",
    "        self.reminders_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"reminders\")\n",
    "        os.makedirs(self.reminders_dir, exist_ok=True)\n",
    "        \n",
    "        # Default configuration\n",
    "        self.config = {\n",
    "            'default_duration': 7,  # Default duration in days\n",
    "            'default_method': 'After Food',  # Default method\n",
    "            'default_status': 'upcoming',  # Default status\n",
    "            'reminder_prefix': 'It\\'s time to take',  # Prefix for reminder text\n",
    "            'reminder_suffix': '',  # Suffix for reminder text\n",
    "            'audio_format': 'mp3',  # Audio format for voice reminders\n",
    "        }\n",
    "        \n",
    "        # Update with user-provided configuration\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "    \n",
    "    def generate_voice_reminder(self, text, reminder_id):\n",
    "        \"\"\"Generate voice reminder for the given text.\"\"\"\n",
    "        try:\n",
    "            output_file = os.path.join(self.reminders_dir, f\"reminder_{reminder_id}.{self.config['audio_format']}\")\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            tts.save(output_file)\n",
    "            return f\"reminder_{reminder_id}.{self.config['audio_format']}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating audio: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def schedule_reminders(self, medicine_data):\n",
    "        \"\"\"\n",
    "        Schedule reminders based on extracted medicine data\n",
    "        \"\"\"\n",
    "        scheduled_reminders = []\n",
    "        \n",
    "        for medicine in medicine_data:\n",
    "            # Extract medicine information\n",
    "            name = medicine.get(\"name\", \"\").strip()\n",
    "            dosage = medicine.get(\"dosage\", \"\").strip()\n",
    "            frequency = medicine.get(\"frequency\", \"\").lower().strip()\n",
    "            duration = medicine.get(\"duration\", \"\").lower().strip()\n",
    "            method = medicine.get(\"method\", self.config['default_method'])\n",
    "            \n",
    "            # Determine times per day\n",
    "            times_per_day = 1\n",
    "            if \"twice\" in frequency or \"2x\" in frequency or \"2 times\" in frequency:\n",
    "                times_per_day = 2\n",
    "            elif \"three\" in frequency or \"3x\" in frequency or \"3 times\" in frequency:\n",
    "                times_per_day = 3\n",
    "            elif \"four\" in frequency or \"4x\" in frequency or \"4 times\" in frequency:\n",
    "                times_per_day = 4\n",
    "            \n",
    "            # Set default reminder times based on times per day\n",
    "            reminder_times = []\n",
    "            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            \n",
    "            if times_per_day == 1:\n",
    "                reminder_times.append(today.replace(hour=9, minute=0))  # 9 AM\n",
    "            elif times_per_day == 2:\n",
    "                reminder_times.append(today.replace(hour=9, minute=0))  # 9 AM\n",
    "                reminder_times.append(today.replace(hour=21, minute=0))  # 9 PM\n",
    "            elif times_per_day == 3:\n",
    "                reminder_times.append(today.replace(hour=9, minute=0))  # 9 AM\n",
    "                reminder_times.append(today.replace(hour=14, minute=0))  # 2 PM\n",
    "                reminder_times.append(today.replace(hour=21, minute=0))  # 9 PM\n",
    "            elif times_per_day == 4:\n",
    "                reminder_times.append(today.replace(hour=8, minute=0))  # 8 AM\n",
    "                reminder_times.append(today.replace(hour=12, minute=0))  # 12 PM\n",
    "                reminder_times.append(today.replace(hour=16, minute=0))  # 4 PM\n",
    "                reminder_times.append(today.replace(hour=20, minute=0))  # 8 PM\n",
    "            \n",
    "            # Determine duration in days\n",
    "            duration_days = self.config['default_duration']  # Default duration\n",
    "            if \"week\" in duration:\n",
    "                try:\n",
    "                    weeks = int(duration.split()[0])\n",
    "                    duration_days = weeks * 7\n",
    "                except:\n",
    "                    pass\n",
    "            elif \"day\" in duration:\n",
    "                try:\n",
    "                    duration_days = int(duration.split()[0])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Set start and end times\n",
    "            start_time = datetime.now()\n",
    "            end_time = start_time + timedelta(days=duration_days)\n",
    "            \n",
    "            # Schedule reminders\n",
    "            for day in range(duration_days):\n",
    "                for base_time in reminder_times:\n",
    "                    reminder_time = base_time + timedelta(days=day)\n",
    "                    if reminder_time > datetime.now() and reminder_time < end_time:\n",
    "                        reminder_id = str(uuid.uuid4())\n",
    "                        reminder_text = f\"{self.config['reminder_prefix']} {dosage} of {name}\"\n",
    "                        if self.config['reminder_suffix']:\n",
    "                            reminder_text += f\" {self.config['reminder_suffix']}\"\n",
    "                        \n",
    "                        # Generate voice reminder\n",
    "                        audio_file = self.generate_voice_reminder(reminder_text, reminder_id)\n",
    "                        \n",
    "                        # Create reminder object\n",
    "                        reminder = {\n",
    "                            'id': reminder_id,\n",
    "                            'user_id': self.user_id,\n",
    "                            'medicine': name,\n",
    "                            'dosage': dosage,\n",
    "                            'frequency': frequency,\n",
    "                            'duration': duration,\n",
    "                            'method': method,\n",
    "                            'reminder_time': reminder_time,\n",
    "                            'status': self.config['default_status'],\n",
    "                            'audio_file': audio_file,\n",
    "                            'prescription_id': self.prescription_id\n",
    "                        }\n",
    "                        \n",
    "                        # Schedule job\n",
    "                        scheduler.add_job(\n",
    "                            func=lambda r=reminder: self.trigger_reminder(r['id']),\n",
    "                            trigger=\"date\",\n",
    "                            run_date=reminder_time,\n",
    "                            id=reminder_id\n",
    "                        )\n",
    "                        \n",
    "                        scheduled_reminders.append(reminder)\n",
    "        \n",
    "        return scheduled_reminders\n",
    "    \n",
    "    def trigger_reminder(self, reminder_id):\n",
    "        \"\"\"\n",
    "        Trigger reminder when time is hit\n",
    "        \"\"\"\n",
    "        print(f\"REMINDER ALERT: Reminder {reminder_id} triggered\")\n",
    "        return reminder_id\n",
    "\n",
    "def create_dataset_structure():\n",
    "    \"\"\"\n",
    "    Create the dataset directory structure for prescription images and labels\n",
    "    \"\"\"\n",
    "    # Define base dataset directory\n",
    "    base_dir = \"dataset\"\n",
    "    \n",
    "    # Create main dataset directory\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for different medicine types\n",
    "    medicine_types = [\n",
    "        \"tablets\",\n",
    "        \"capsules\",\n",
    "        \"syrups\",\n",
    "        \"injections\",\n",
    "        \"drops\",\n",
    "        \"creams\",\n",
    "        \"inhalers\"\n",
    "    ]\n",
    "    \n",
    "    for med_type in medicine_types:\n",
    "        os.makedirs(os.path.join(base_dir, med_type), exist_ok=True)\n",
    "    \n",
    "    # Create labels directory\n",
    "    labels_dir = os.path.join(base_dir, \"labels\")\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a sample label file\n",
    "    sample_label = {\n",
    "        \"image_id\": \"sample_image\",\n",
    "        \"medicine_types\": [1, 0, 0, 0, 0, 0, 0],  # Example: tablet\n",
    "        \"medicines\": [\n",
    "            {\n",
    "                \"name\": \"Sample Medicine\",\n",
    "                \"dosage\": \"1 tablet\",\n",
    "                \"frequency\": \"twice a day\",\n",
    "                \"duration\": \"7 days\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(labels_dir, \"sample_label.json\"), \"w\") as f:\n",
    "        json.dump(sample_label, f, indent=4)\n",
    "    \n",
    "    print(f\"Dataset structure created at: {os.path.abspath(base_dir)}\")\n",
    "    print(\"Please place your prescription images in the appropriate subdirectories.\")\n",
    "    print(\"For example, tablet prescriptions should go in the 'tablets' directory.\")\n",
    "    print(\"Label files should be placed in the 'labels' directory.\")\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # GPU setup and memory management\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.cuda.empty_cache()\n",
    "            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            logger.info(\"Using CPU\")\n",
    "        \n",
    "        # Initialize OCR\n",
    "        logger.info(\"Initializing OCR...\")\n",
    "        ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "        \n",
    "        # Initialize NLP components\n",
    "        logger.info(\"Initializing NLP components...\")\n",
    "        text_extractor = NLPTextExtractor()\n",
    "        \n",
    "        # Define fixed dataset path\n",
    "        dataset_path = \"D:/Coding/Machine_Learning/Projects/VAMD/dataset/train\"\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n=== Prescription Processing System ===\")\n",
    "            print(\"1. Process prescription image\")\n",
    "            print(\"2. Train model\")\n",
    "            print(\"3. Create dataset structure\")\n",
    "            print(\"4. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-4): \")\n",
    "            \n",
    "            if choice == \"4\":\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "                \n",
    "            elif choice == \"3\":\n",
    "                # Create dataset structure\n",
    "                print(\"\\n=== Creating Dataset Structure ===\")\n",
    "                dataset_path = create_dataset_structure()\n",
    "                print(f\"Dataset structure created at: {dataset_path}\")\n",
    "                continue\n",
    "                \n",
    "            elif choice == \"2\":\n",
    "                # Train model\n",
    "                print(\"\\n=== Model Training ===\")\n",
    "                \n",
    "                if not os.path.exists(dataset_path):\n",
    "                    print(\"Error: Dataset directory does not exist!\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if directory is empty\n",
    "                image_files = [f for f in os.listdir(os.path.join(dataset_path, \"resized_images\")) \n",
    "                             if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                if not image_files:\n",
    "                    print(\"Error: No image files found in the dataset directory!\")\n",
    "                    continue\n",
    "                \n",
    "                # Define transformations\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "                \n",
    "                try:\n",
    "                    # Create dataset\n",
    "                    dataset = PrescriptionDataset(\n",
    "                        image_dir=dataset_path,\n",
    "                        transform=transform,\n",
    "                        cache_dir=\"ocr_cache\"\n",
    "                    )\n",
    "                    \n",
    "                    # Check if dataset is empty\n",
    "                    if len(dataset) == 0:\n",
    "                        print(\"Error: Dataset is empty!\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Split dataset\n",
    "                    train_size = int(0.8 * len(dataset))\n",
    "                    val_size = len(dataset) - train_size\n",
    "                    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "                    \n",
    "                    # Create data loaders\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    model = PrescriptionModel()\n",
    "                    \n",
    "                    # Initialize trainer\n",
    "                    trainer = PrescriptionTrainer(\n",
    "                        model=model,\n",
    "                        train_loader=train_loader,\n",
    "                        val_loader=val_loader,\n",
    "                        device=device\n",
    "                    )\n",
    "                    \n",
    "                    # Train model\n",
    "                    trainer.train(num_epochs=10, patience=3)\n",
    "                    \n",
    "                    print(\"Model training completed!\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during model training: {str(e)}\")\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "            elif choice == \"1\":\n",
    "                # Get prescription image path from user\n",
    "                image_path = input(\"\\nEnter the path to your prescription image: \")\n",
    "                \n",
    "                if not os.path.exists(image_path):\n",
    "                    print(\"Error: File does not exist!\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Process the image with OCR\n",
    "                    logger.info(f\"Processing image: {image_path}\")\n",
    "                    result = ocr.ocr(image_path, cls=True)\n",
    "                    \n",
    "                    if not result or not result[0]:\n",
    "                        print(\"Error: Could not extract text from the image!\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract text from OCR result\n",
    "                    extracted_text = \"\\n\".join([line[1][0] for line in result[0]])\n",
    "                    print(\"\\nExtracted Text:\")\n",
    "                    print(\"---------------\")\n",
    "                    print(extracted_text)\n",
    "                    \n",
    "                    # Extract medicine data using NLP\n",
    "                    logger.info(\"Extracting medicine data...\")\n",
    "                    extracted_medicine_data = text_extractor.extract_medicine_data(extracted_text)\n",
    "                    \n",
    "                    if not extracted_medicine_data:\n",
    "                        print(\"\\nNo medicine information found in the prescription!\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(\"\\nExtracted Medicine Data:\")\n",
    "                    print(\"----------------------\")\n",
    "                    for idx, medicine in enumerate(extracted_medicine_data, 1):\n",
    "                        print(f\"\\nMedicine {idx}:\")\n",
    "                        print(f\"Name: {medicine.get('name', 'N/A')}\")\n",
    "                        print(f\"Dosage: {medicine.get('dosage', 'N/A')}\")\n",
    "                        print(f\"Frequency: {medicine.get('frequency', 'N/A')}\")\n",
    "                        print(f\"Duration: {medicine.get('duration', 'N/A')}\")\n",
    "                    \n",
    "                    # Ask user if they want to schedule reminders\n",
    "                    schedule_choice = input(\"\\nWould you like to schedule reminders for these medicines? (y/n): \")\n",
    "                    \n",
    "                    if schedule_choice.lower() == 'y':\n",
    "                        # Get user ID (in a real application, this would come from authentication)\n",
    "                        user_id = input(\"Enter your user ID: \")\n",
    "                        \n",
    "                        # Initialize reminder generator\n",
    "                        logger.info(\"Initializing reminder generator...\")\n",
    "                        reminder_generator = ReminderGenerator(user_id=user_id)\n",
    "                        \n",
    "                        # Schedule reminders\n",
    "                        logger.info(\"Scheduling reminders...\")\n",
    "                        scheduled_reminders = reminder_generator.schedule_reminders(extracted_medicine_data)\n",
    "                        \n",
    "                        print(f\"\\nSuccessfully scheduled {len(scheduled_reminders)} reminders!\")\n",
    "                        print(\"\\nReminder Schedule:\")\n",
    "                        print(\"-----------------\")\n",
    "                        for reminder in scheduled_reminders:\n",
    "                            print(f\"\\nMedicine: {reminder['medicine']}\")\n",
    "                            print(f\"Dosage: {reminder['dosage']}\")\n",
    "                            print(f\"Time: {reminder['reminder_time'].strftime('%Y-%m-%d %H:%M')}\")\n",
    "                            print(f\"Status: {reminder['status']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing prescription: {str(e)}\")\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "            \n",
    "            else:\n",
    "                print(\"Invalid choice! Please try again.\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
